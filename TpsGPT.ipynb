{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning ProtGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/nferruz/ProtGPT2\n",
    "# https://huggingface.co/littleworth/protgpt2-distilled-tiny\n",
    "!python run_clm.py --model_name_or_path littleworth/protgpt2-distilled-tiny \\\n",
    "--train_file training.txt \\\n",
    "--validation_file validation_M6.txt \\\n",
    "--tokenizer_name littleworth/protgpt2-distilled-tiny \\\n",
    "--do_train --do_eval \\\n",
    "--eval_strategy epoch \\\n",
    "--output_dir ./output_tiny/ \\\n",
    "--learning_rate 1e-4 \\\n",
    "--overwrite_output_dir \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--per_device_eval_batch_size 64 \\\n",
    "--block_size 512 \\\n",
    "--max_steps 4000 \\\n",
    "--gradient_accumulation_steps 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPS Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# https://huggingface.co/nferruz/ProtGPT2\n",
    "# https://huggingface.co/nferruz/ProtGPT2/discussions/38\n",
    "\n",
    "TOTAL_SEQUENCES = 1000\n",
    "NUM_SEQ_PER_GENERATION = 50\n",
    "\n",
    "model_path = \"./output_tiny/\"\n",
    "device = 'cuda'\n",
    "\n",
    "def calculatePerplexity(sequence, model, tokenizer):\n",
    "    input_ids = torch.tensor(tokenizer.encode(sequence)).unsqueeze(0) \n",
    "    input_ids = input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "    loss, logits = outputs[:2]\n",
    "    return math.exp(loss)\n",
    "\n",
    "def sort_protein_sequences(sequences, scores, top_n=1000):\n",
    "    \"\"\"\n",
    "    Sorts protein sequences by score and returns the top N sequences.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of protein sequences.\n",
    "        scores (list): List of scores corresponding to protein sequences.\n",
    "        top_n (int, optional): Number of top sequences to return. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        list: Top N protein sequences along with their scores.\n",
    "    \"\"\"\n",
    "    # Combine sequences and scores into a list of tuples\n",
    "    sequence_score_pairs = list(zip(sequences, scores))\n",
    "    # Sort the list in ascending order based on scores\n",
    "    sorted_pairs = sorted(sequence_score_pairs, key=lambda x: x[1], reverse=False)\n",
    "    return sorted_pairs[:top_n]\n",
    "\n",
    "# ProtGPT2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "# ProtGPT2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "# protgpt2 pipeline to generate artificial TPS sequences from the fine tuned model\n",
    "protgpt2 = pipeline('text-generation', model=model_path, device=device)\n",
    "\n",
    " # integer division\n",
    "num_iter = TOTAL_SEQUENCES // NUM_SEQ_PER_GENERATION\n",
    "# generated TPS proteins\n",
    "tps_proteins = []\n",
    "# perplexity scores for corresponding proteins\n",
    "ppl_scores = []\n",
    "# generate sequences of different max_length tokens in increments of 10 starting from 70\n",
    "for i in range(14):\n",
    "    max_len_var = 70 + (10*i)\n",
    "    for i in range(0, num_iter):\n",
    "        # generate sequences given the <|endoftext|> prompt\n",
    "        sequences = protgpt2(\n",
    "            \"<|endoftext|>\", \n",
    "            max_length=max_len_var, # length is expressed in tokens, where each token has an average length of 4 amino acids.\n",
    "            do_sample=True, \n",
    "            top_k=950, \n",
    "            repetition_penalty=1.2, \n",
    "            num_return_sequences=NUM_SEQ_PER_GENERATION, \n",
    "            pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to eos_token_id\n",
    "            eos_token_id=0, \n",
    "            truncation=True)\n",
    "        # compute perplexity scores for each generated sequence and store them\n",
    "        for seq in sequences:\n",
    "            print(seq)\n",
    "            tmp_seq = seq['generated_text'].rstrip() # remove right space\n",
    "            tps_proteins.append(tmp_seq + '\\n')\n",
    "            modified_seq_padded = tmp_seq + '<|endoftext|>'\n",
    "            ppl = calculatePerplexity(modified_seq_padded, model, tokenizer)\n",
    "            print(\"Generated \", str(len(tps_proteins)), \" sequences\")\n",
    "            ppl_scores.append(ppl)\n",
    "\n",
    "# pick top 10%\n",
    "top_sequences = sort_protein_sequences(tps_proteins, ppl_scores, top_n=1400)\n",
    "\n",
    "# write the generated sequences to file\n",
    "output_seq = []\n",
    "output_ppl_scores = []\n",
    "for sequence, score in top_sequences:\n",
    "    output_seq.append(sequence)\n",
    "    output_ppl_scores.append(round(score, 2))\n",
    "\n",
    "output_file_path = \"top_tps_sequences.txt\"\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for item in output_seq:\n",
    "         file.write(item.rstrip() + '\\n')\n",
    "\n",
    "output_file_path = \"perplexity_scores.txt\"\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for item in output_ppl_scores:\n",
    "         file.write(str(item) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
